# Exerc√≠cio 4: Estrat√©gias Cross-Region e Limita√ß√µes

## üéØ Objetivos

- Explorar limita√ß√µes do DocumentDB para replica√ß√£o cross-region
- Implementar estrat√©gias alternativas de sincroniza√ß√£o entre regi√µes
- Configurar arquiteturas multi-regi√£o resilientes
- Desenvolver planos de failover regional

## ‚è±Ô∏è Dura√ß√£o Estimada
105 minutos

> ‚ö†Ô∏è **Aten√ß√£o:** Lembre-se de usar seu prefixo de aluno (`<seu-id>`) em todos os nomes de recursos e comandos.

---

## üåç Parte 1: An√°lise de Limita√ß√µes Cross-Region

### Passo 1: Documentar Limita√ß√µes do DocumentDB

```bash
# Configurar vari√°veis
export ID="<seu-id>"
export PRIMARY_REGION="us-east-1"
export SECONDARY_REGION="us-west-2"
export CLUSTER_ID="$ID-lab-cluster-console"

# Criar documento de limita√ß√µes
cat > architectures/documentdb-limitations.md << 'EOF'
# Limita√ß√µes do DocumentDB para Cross-Region

## Limita√ß√µes Nativas

### 1. Replica√ß√£o Cross-Region
- ‚ùå **N√£o suportada nativamente**
- ‚ùå N√£o h√° read replicas cross-region autom√°ticas
- ‚ùå N√£o h√° sincroniza√ß√£o autom√°tica entre regi√µes

### 2. Backup Cross-Region
- ‚úÖ **Snapshots podem ser copiados entre regi√µes**
- ‚ö†Ô∏è Processo manual ou via automa√ß√£o customizada
- ‚ö†Ô∏è Custos de transfer√™ncia de dados aplic√°veis

### 3. Failover Regional
- ‚ùå **N√£o h√° failover autom√°tico entre regi√µes**
- ‚ö†Ô∏è Requer interven√ß√£o manual ou automa√ß√£o customizada
- ‚ö†Ô∏è RTO pode ser alto (horas) sem prepara√ß√£o adequada

## Compara√ß√£o com RDS/Aurora

| Recurso | DocumentDB | RDS/Aurora |
|---------|------------|------------|
| Cross-Region Read Replicas | ‚ùå | ‚úÖ |
| Global Database | ‚ùå | ‚úÖ (Aurora) |
| Automated Cross-Region Backup | ‚ùå | ‚úÖ |
| Cross-Region Failover | ‚ùå | ‚úÖ (Aurora) |

## Implica√ß√µes Arquiteturais

### Para Alta Disponibilidade
- Depend√™ncia de uma √∫nica regi√£o
- Necessidade de estrat√©gias customizadas
- Maior complexidade operacional

### Para Disaster Recovery
- RPO potencialmente alto
- RTO dependente de processos manuais
- Necessidade de automa√ß√£o customizada

### Para Performance Global
- Lat√™ncia alta para usu√°rios distantes
- Impossibilidade de distribui√ß√£o geogr√°fica nativa
- Necessidade de arquiteturas alternativas
EOF
```

### Passo 2: Avaliar Alternativas Arquiteturais

```bash
# Criar an√°lise de alternativas
cat > architectures/multi-region-design.md << 'EOF'
# Estrat√©gias Multi-Regi√£o para DocumentDB

## Estrat√©gia 1: Snapshot Cross-Region (Disaster Recovery)

### Arquitetura
```
Primary Region (us-east-1)     Secondary Region (us-west-2)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DocumentDB Cluster     ‚îÇ    ‚îÇ  Standby Infrastructure ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Primary Instance   ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ VPC               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Read Replica 1     ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ Subnets           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Read Replica 2     ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ Security Groups   ‚îÇ
‚îÇ                         ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ Parameter Groups  ‚îÇ
‚îÇ  Automated Snapshots    ‚îÇ    ‚îÇ                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Daily: 2:00 AM     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Cross-Region Snapshots ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Hourly: On-demand  ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ Daily Copies      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îî‚îÄ‚îÄ Emergency Copies   ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Caracter√≠sticas
- **RPO:** 1-24 horas (dependendo da frequ√™ncia)
- **RTO:** 2-4 horas (restaura√ß√£o + configura√ß√£o)
- **Custo:** Baixo (apenas snapshots + storage)
- **Complexidade:** M√©dia

## Estrat√©gia 2: Aplica√ß√£o Dual-Write (Active-Active)

### Arquitetura
```
Application Layer
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Load Balancer / API Gateway                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Route 53 Health Checks                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Failover Routing                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ
Primary Region         Secondary Region
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DocumentDB      ‚îÇ    ‚îÇ DocumentDB      ‚îÇ
‚îÇ Cluster A       ‚îÇ    ‚îÇ Cluster B       ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ Application     ‚îÇ    ‚îÇ Application     ‚îÇ
‚îÇ writes to both  ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∂‚îÇ writes to both  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Caracter√≠sticas
- **RPO:** Pr√≥ximo de zero
- **RTO:** Segundos (failover de DNS)
- **Custo:** Alto (clusters duplos)
- **Complexidade:** Alta (conflict resolution)

## Estrat√©gia 3: Change Data Capture (CDC)

### Arquitetura
```
Primary Region                 Secondary Region
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DocumentDB Cluster     ‚îÇ    ‚îÇ  DocumentDB Cluster     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Primary Instance   ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ Primary Instance   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Read Replicas      ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ Read Replicas      ‚îÇ
‚îÇ                         ‚îÇ    ‚îÇ                         ‚îÇ
‚îÇ  Change Stream          ‚îÇ    ‚îÇ                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Lambda Function    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Replication Lambda     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ DynamoDB Streams   ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ Conflict Detection ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Kinesis Data       ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ Data Validation    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Caracter√≠sticas
- **RPO:** Minutos
- **RTO:** Minutos (autom√°tico)
- **Custo:** M√©dio (processamento + transfer√™ncia)
- **Complexidade:** Alta (CDC implementation)
EOF
```

---

## üìã Parte 2: Implementa√ß√£o de Snapshot Cross-Region

### Passo 1: Configurar Infraestrutura na Regi√£o Secund√°ria

```bash
# Criar VPC na regi√£o secund√°ria
aws ec2 create-vpc \
--region $SECONDARY_REGION \
--cidr-block 10.1.0.0/16 \
--tag-specifications 'ResourceType=vpc,Tags=[{Key=Name,Value='$ID'-docdb-vpc-secondary}]'

# Obter VPC ID
SECONDARY_VPC_ID=$(aws ec2 describe-vpcs \
--region $SECONDARY_REGION \
--filters "Name=tag:Name,Values=$ID-docdb-vpc-secondary" \
--query 'Vpcs[0].VpcId' \
--output text)

# Criar subnets em AZs diferentes
SECONDARY_AZS=($(aws ec2 describe-availability-zones \
--region $SECONDARY_REGION \
--query 'AvailabilityZones[0:2].ZoneName' \
--output text))

# Subnet 1
aws ec2 create-subnet \
--region $SECONDARY_REGION \
--vpc-id $SECONDARY_VPC_ID \
--cidr-block 10.1.1.0/24 \
--availability-zone ${SECONDARY_AZS[0]} \
--tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value='$ID'-docdb-subnet-secondary-1}]'

# Subnet 2
aws ec2 create-subnet \
--region $SECONDARY_REGION \
--vpc-id $SECONDARY_VPC_ID \
--cidr-block 10.1.2.0/24 \
--availability-zone ${SECONDARY_AZS[1]} \
--tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value='$ID'-docdb-subnet-secondary-2}]'

# Criar DB Subnet Group
SECONDARY_SUBNET_IDS=$(aws ec2 describe-subnets \
--region $SECONDARY_REGION \
--filters "Name=vpc-id,Values=$SECONDARY_VPC_ID" \
--query 'Subnets[].SubnetId' \
--output text)

aws docdb create-db-subnet-group \
--region $SECONDARY_REGION \
--db-subnet-group-name $ID-docdb-subnet-group-secondary \
--db-subnet-group-description "Subnet group for DocumentDB in secondary region" \
--subnet-ids $SECONDARY_SUBNET_IDS

echo "Infraestrutura secund√°ria criada na regi√£o $SECONDARY_REGION"
```

### Passo 2: Automa√ß√£o de C√≥pia de Snapshots

```python
# Criar fun√ß√£o Lambda para c√≥pia cross-region
cat > lambda/cross-region-backup.py << 'EOF'
import json
import boto3
from datetime import datetime, timedelta

def lambda_handler(event, context):
    """
    Fun√ß√£o para copiar snapshots do DocumentDB entre regi√µes
    """
    
    source_region = event.get('source_region', 'us-east-1')
    target_region = event.get('target_region', 'us-west-2')
    cluster_identifier = event['cluster_identifier']
    retention_days = event.get('retention_days', 7)
    
    # Clientes para ambas as regi√µes
    source_docdb = boto3.client('docdb', region_name=source_region)
    target_docdb = boto3.client('docdb', region_name=target_region)
    
    try:
        # Listar snapshots autom√°ticos na regi√£o source
        response = source_docdb.describe_db_cluster_snapshots(
            DBClusterIdentifier=cluster_identifier,
            SnapshotType='automated',
            MaxRecords=50
        )
        
        snapshots = response['DBClusterSnapshots']
        
        # Filtrar snapshots das √∫ltimas 24 horas
        yesterday = datetime.now() - timedelta(days=1)
        recent_snapshots = [
            s for s in snapshots 
            if s['SnapshotCreateTime'].replace(tzinfo=None) > yesterday
        ]
        
        copied_snapshots = []
        
        for snapshot in recent_snapshots:
            source_snapshot_id = snapshot['DBClusterSnapshotIdentifier']
            source_snapshot_arn = snapshot['DBClusterSnapshotArn']
            
            # Gerar ID para snapshot de destino
            target_snapshot_id = f"{cluster_identifier}-cross-region-{snapshot['SnapshotCreateTime'].strftime('%Y%m%d%H%M%S')}"
            
            # Verificar se j√° existe na regi√£o de destino
            try:
                target_docdb.describe_db_cluster_snapshots(
                    DBClusterSnapshotIdentifier=target_snapshot_id
                )
                print(f"Snapshot {target_snapshot_id} j√° existe na regi√£o de destino")
                continue
            except target_docdb.exceptions.DBClusterSnapshotNotFoundFault:
                pass
            
            # Copiar snapshot
            print(f"Copiando snapshot {source_snapshot_id} para {target_region}")
            
            copy_response = target_docdb.copy_db_cluster_snapshot(
                SourceDBClusterSnapshotIdentifier=source_snapshot_arn,
                TargetDBClusterSnapshotIdentifier=target_snapshot_id,
                CopyTags=True
            )
            
            copied_snapshots.append({
                'source_id': source_snapshot_id,
                'target_id': target_snapshot_id,
                'status': 'copying'
            })
        
        # Limpar snapshots antigos na regi√£o de destino
        cleanup_response = target_docdb.describe_db_cluster_snapshots(
            SnapshotType='manual',
            MaxRecords=100
        )
        
        cleanup_date = datetime.now() - timedelta(days=retention_days)
        
        for old_snapshot in cleanup_response['DBClusterSnapshots']:
            if (old_snapshot['DBClusterSnapshotIdentifier'].startswith(f"{cluster_identifier}-cross-region") and
                old_snapshot['SnapshotCreateTime'].replace(tzinfo=None) < cleanup_date):
                
                print(f"Removendo snapshot antigo: {old_snapshot['DBClusterSnapshotIdentifier']}")
                
                target_docdb.delete_db_cluster_snapshot(
                    DBClusterSnapshotIdentifier=old_snapshot['DBClusterSnapshotIdentifier']
                )
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Cross-region backup completed successfully',
                'copied_snapshots': copied_snapshots,
                'source_region': source_region,
                'target_region': target_region
            })
        }
        
    except Exception as e:
        print(f"Error in cross-region backup: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e)
            })
        }
EOF
```

### Passo 3: Configurar Agendamento Cross-Region

```bash
# Criar fun√ß√£o Lambda para backup cross-region
zip -j cross-region-backup.zip lambda/cross-region-backup.py

aws lambda create-function \
--region $PRIMARY_REGION \
--function-name $ID-CrossRegionBackup \
--runtime python3.9 \
--role arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/$ID-DocumentDBExportRole \
--handler cross-region-backup.lambda_handler \
--zip-file fileb://cross-region-backup.zip \
--timeout 900

# Criar regra EventBridge para execu√ß√£o di√°ria
aws events put-rule \
--region $PRIMARY_REGION \
--name $ID-cross-region-backup \
--description "Backup cross-region di√°rio do DocumentDB" \
--schedule-expression "cron(0 3 * * ? *)"  # Todo dia √†s 3:00 AM UTC

# Configurar target
aws events put-targets \
--region $PRIMARY_REGION \
--rule $ID-cross-region-backup \
--targets "Id"="1","Arn"="arn:aws:lambda:$PRIMARY_REGION:$(aws sts get-caller-identity --query Account --output text):function:$ID-CrossRegionBackup","Input"="{\"cluster_identifier\":\"$CLUSTER_ID\",\"source_region\":\"$PRIMARY_REGION\",\"target_region\":\"$SECONDARY_REGION\"}"

echo "Backup cross-region configurado"
```

---

## üîÑ Parte 3: Implementa√ß√£o de Sincroniza√ß√£o Customizada

### Passo 1: Change Data Capture com Lambda

```javascript
// Implementar CDC b√°sico
cat > scripts/cross-region-sync.js << 'EOF'
const { MongoClient } = require('mongodb');
const AWS = require('aws-sdk');

class CrossRegionSync {
  constructor(primaryEndpoint, secondaryEndpoint, credentials) {
    this.primaryClient = new MongoClient(`mongodb://${credentials.username}:${credentials.password}@${primaryEndpoint}:27017/syncDB?ssl=true&retryWrites=false`);
    this.secondaryClient = new MongoClient(`mongodb://${credentials.username}:${credentials.password}@${secondaryEndpoint}:27017/syncDB?ssl=true&retryWrites=false`);
    
    this.syncLog = new Map(); // Track sync status
    this.conflictResolver = new ConflictResolver();
  }

  async connect() {
    await Promise.all([
      this.primaryClient.connect(),
      this.secondaryClient.connect()
    ]);
    console.log('Connected to both regions');
  }

  async startSync(collections = ['products', 'orders']) {
    console.log('Starting cross-region synchronization...');
    
    for (const collectionName of collections) {
      await this.syncCollection(collectionName);
    }
  }

  async syncCollection(collectionName) {
    const primaryDb = this.primaryClient.db('syncDB');
    const secondaryDb = this.secondaryClient.db('syncDB');
    
    const primaryCollection = primaryDb.collection(collectionName);
    const secondaryCollection = secondaryDb.collection(collectionName);
    
    // Implementar change stream no primary
    const changeStream = primaryCollection.watch([], {
      fullDocument: 'updateLookup'
    });
    
    console.log(`Watching changes on ${collectionName}...`);
    
    changeStream.on('change', async (change) => {
      try {
        await this.processChange(change, secondaryCollection);
      } catch (error) {
        console.error(`Error processing change for ${collectionName}:`, error);
        await this.handleSyncError(change, error);
      }
    });
    
    // Sync inicial (full sync)
    await this.performInitialSync(primaryCollection, secondaryCollection);
  }

  async processChange(change, targetCollection) {
    const { operationType, documentKey, fullDocument } = change;
    
    switch (operationType) {
      case 'insert':
        await this.handleInsert(fullDocument, targetCollection);
        break;
      case 'update':
        await this.handleUpdate(documentKey, fullDocument, targetCollection);
        break;
      case 'delete':
        await this.handleDelete(documentKey, targetCollection);
        break;
      case 'replace':
        await this.handleReplace(documentKey, fullDocument, targetCollection);
        break;
    }
    
    // Log sync operation
    this.logSyncOperation(change);
  }

  async handleInsert(document, targetCollection) {
    // Check for conflicts
    const existing = await targetCollection.findOne({_id: document._id});
    
    if (existing) {
      // Conflict detected
      const resolution = await this.conflictResolver.resolve('insert', document, existing);
      if (resolution.action === 'overwrite') {
        await targetCollection.replaceOne({_id: document._id}, document);
      }
    } else {
      await targetCollection.insertOne(document);
    }
  }

  async handleUpdate(documentKey, fullDocument, targetCollection) {
    if (fullDocument) {
      await targetCollection.replaceOne(
        {_id: documentKey._id},
        fullDocument,
        {upsert: true}
      );
    }
  }

  async handleDelete(documentKey, targetCollection) {
    await targetCollection.deleteOne({_id: documentKey._id});
  }

  async handleReplace(documentKey, fullDocument, targetCollection) {
    await targetCollection.replaceOne(
      {_id: documentKey._id},
      fullDocument,
      {upsert: true}
    );
  }

  async performInitialSync(sourceCollection, targetCollection) {
    console.log('Performing initial sync...');
    
    const cursor = sourceCollection.find({});
    let syncedCount = 0;
    
    while (await cursor.hasNext()) {
      const doc = await cursor.next();
      
      try {
        await targetCollection.replaceOne(
          {_id: doc._id},
          doc,
          {upsert: true}
        );
        syncedCount++;
        
        if (syncedCount % 1000 === 0) {
          console.log(`Synced ${syncedCount} documents...`);
        }
      } catch (error) {
        console.error(`Error syncing document ${doc._id}:`, error);
      }
    }
    
    console.log(`Initial sync completed: ${syncedCount} documents`);
  }

  logSyncOperation(change) {
    const logEntry = {
      timestamp: new Date(),
      operationType: change.operationType,
      documentId: change.documentKey._id,
      clusterTime: change.clusterTime
    };
    
    this.syncLog.set(change.documentKey._id.toString(), logEntry);
  }

  async handleSyncError(change, error) {
    // Implement error handling and retry logic
    console.error('Sync error:', {
      change: change,
      error: error.message
    });
    
    // Send to DLQ or retry queue
    // Implement exponential backoff
  }

  async getSyncStatus() {
    return {
      totalOperations: this.syncLog.size,
      lastSync: Array.from(this.syncLog.values()).pop()?.timestamp,
      errors: 0 // Implement error tracking
    };
  }
}

class ConflictResolver {
  async resolve(operation, newDoc, existingDoc) {
    // Implement conflict resolution strategies
    
    // Strategy 1: Last Write Wins (based on timestamp)
    if (newDoc.lastModified && existingDoc.lastModified) {
      if (newDoc.lastModified > existingDoc.lastModified) {
        return { action: 'overwrite', document: newDoc };
      } else {
        return { action: 'ignore', document: existingDoc };
      }
    }
    
    // Strategy 2: Primary Region Wins
    return { action: 'overwrite', document: newDoc };
  }
}

// CLI interface
async function main() {
  const primaryEndpoint = process.env.PRIMARY_ENDPOINT;
  const secondaryEndpoint = process.env.SECONDARY_ENDPOINT;
  const credentials = {
    username: process.env.DB_USERNAME,
    password: process.env.DB_PASSWORD
  };

  const sync = new CrossRegionSync(primaryEndpoint, secondaryEndpoint, credentials);
  
  try {
    await sync.connect();
    await sync.startSync(['products', 'orders']);
    
    // Keep running
    process.on('SIGINT', async () => {
      console.log('Shutting down sync...');
      await sync.primaryClient.close();
      await sync.secondaryClient.close();
      process.exit(0);
    });
    
  } catch (error) {
    console.error('Sync failed:', error);
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = CrossRegionSync;
EOF
```

---

## üö® Parte 4: Plano de Failover Regional

### Passo 1: Script de Failover Autom√°tico

```bash
# Criar script de failover regional
cat > scripts/region-failover.sh << 'EOF'
#!/bin/bash

# Script de failover regional para DocumentDB
# Uso: ./region-failover.sh <cluster-id> <primary-region> <secondary-region>

CLUSTER_ID=$1
PRIMARY_REGION=$2
SECONDARY_REGION=$3
NOTIFICATION_TOPIC=$4

if [ $# -lt 3 ]; then
    echo "Uso: $0 <cluster-id> <primary-region> <secondary-region> [notification-topic]"
    exit 1
fi

echo "=== INICIANDO FAILOVER REGIONAL ==="
echo "Cluster: $CLUSTER_ID"
echo "Primary Region: $PRIMARY_REGION"
echo "Secondary Region: $SECONDARY_REGION"
echo "Timestamp: $(date)"

# Fun√ß√£o para enviar notifica√ß√£o
send_notification() {
    local message="$1"
    local subject="$2"
    
    if [ ! -z "$NOTIFICATION_TOPIC" ]; then
        aws sns publish \
        --topic-arn $NOTIFICATION_TOPIC \
        --subject "$subject" \
        --message "$message"
    fi
    
    echo "$message"
}

# 1. Verificar status da regi√£o prim√°ria
echo "1. Verificando status da regi√£o prim√°ria..."
PRIMARY_STATUS=$(aws docdb describe-db-clusters \
--region $PRIMARY_REGION \
--db-cluster-identifier $CLUSTER_ID \
--query 'DBClusters[0].Status' \
--output text 2>/dev/null || echo "UNAVAILABLE")

if [ "$PRIMARY_STATUS" = "available" ]; then
    echo "   ‚ö†Ô∏è  Regi√£o prim√°ria ainda dispon√≠vel. Confirme se failover √© necess√°rio."
    read -p "   Continuar com failover? (y/N): " confirm
    if [ "$confirm" != "y" ]; then
        echo "   Failover cancelado pelo usu√°rio."
        exit 0
    fi
fi

send_notification "Iniciando failover regional para $CLUSTER_ID" "DocumentDB Regional Failover Started"

# 2. Encontrar snapshot mais recente na regi√£o secund√°ria
echo "2. Localizando snapshot mais recente na regi√£o secund√°ria..."
LATEST_SNAPSHOT=$(aws docdb describe-db-cluster-snapshots \
--region $SECONDARY_REGION \
--query "DBClusterSnapshots[?contains(DBClusterSnapshotIdentifier, '$CLUSTER_ID-cross-region')].{ID:DBClusterSnapshotIdentifier,Time:SnapshotCreateTime}" \
--output text | sort -k2 -r | head -1 | cut -f1)

if [ -z "$LATEST_SNAPSHOT" ]; then
    echo "   ‚ùå Nenhum snapshot encontrado na regi√£o secund√°ria!"
    send_notification "ERRO: Nenhum snapshot dispon√≠vel para failover" "DocumentDB Failover Failed"
    exit 1
fi

echo "   ‚úÖ Snapshot encontrado: $LATEST_SNAPSHOT"

# 3. Restaurar cluster na regi√£o secund√°ria
echo "3. Restaurando cluster na regi√£o secund√°ria..."
FAILOVER_CLUSTER_ID="${CLUSTER_ID}-failover-$(date +%Y%m%d%H%M%S)"

START_TIME=$(date +%s)

aws docdb restore-db-cluster-from-snapshot \
--region $SECONDARY_REGION \
--db-cluster-identifier $FAILOVER_CLUSTER_ID \
--snapshot-identifier $LATEST_SNAPSHOT \
--engine docdb \
--db-subnet-group-name $ID-docdb-subnet-group-secondary

# 4. Criar inst√¢ncias no cluster restaurado
echo "4. Criando inst√¢ncias no cluster restaurado..."
aws docdb create-db-instance \
--region $SECONDARY_REGION \
--db-instance-identifier ${FAILOVER_CLUSTER_ID}-1 \
--db-instance-class db.t3.medium \
--db-cluster-identifier $FAILOVER_CLUSTER_ID \
--engine docdb

# Aguardar disponibilidade
echo "5. Aguardando cluster ficar dispon√≠vel..."
aws docdb wait db-cluster-available \
--region $SECONDARY_REGION \
--db-cluster-identifier $FAILOVER_CLUSTER_ID

END_TIME=$(date +%s)
FAILOVER_TIME=$((END_TIME - START_TIME))

# 6. Obter endpoint do novo cluster
NEW_ENDPOINT=$(aws docdb describe-db-clusters \
--region $SECONDARY_REGION \
--db-cluster-identifier $FAILOVER_CLUSTER_ID \
--query 'DBClusters[0].Endpoint' \
--output text)

echo "6. Cluster restaurado com sucesso!"
echo "   ‚úÖ Novo Cluster ID: $FAILOVER_CLUSTER_ID"
echo "   ‚úÖ Novo Endpoint: $NEW_ENDPOINT"
echo "   ‚úÖ Regi√£o: $SECONDARY_REGION"
echo "   ‚úÖ Tempo de Failover: ${FAILOVER_TIME}s"

# 7. Atualizar DNS (se configurado)
if [ ! -z "$ROUTE53_HOSTED_ZONE" ] && [ ! -z "$DNS_RECORD" ]; then
    echo "7. Atualizando DNS..."
    # Implementar atualiza√ß√£o Route53
    echo "   DNS atualizado para apontar para nova regi√£o"
fi

# 8. Notificar conclus√£o
COMPLETION_MESSAGE="Failover regional conclu√≠do com sucesso!

Detalhes:
- Cluster Original: $CLUSTER_ID ($PRIMARY_REGION)
- Novo Cluster: $FAILOVER_CLUSTER_ID ($SECONDARY_REGION)
- Novo Endpoint: $NEW_ENDPOINT
- Tempo de Failover: ${FAILOVER_TIME}s
- Snapshot Usado: $LATEST_SNAPSHOT

Pr√≥ximos Passos:
1. Atualizar aplica√ß√µes para usar novo endpoint
2. Validar integridade dos dados
3. Monitorar performance na nova regi√£o
4. Planejar failback quando apropriado"

send_notification "$COMPLETION_MESSAGE" "DocumentDB Regional Failover Completed"

echo ""
echo "=== FAILOVER REGIONAL CONCLU√çDO ==="
echo "Novo endpoint: $NEW_ENDPOINT"
echo "Tempo total: ${FAILOVER_TIME}s"
EOF

chmod +x scripts/region-failover.sh
```

### Passo 2: Valida√ß√£o de Integridade P√≥s-Failover

```javascript
// Script de valida√ß√£o p√≥s-failover
cat > scripts/post-failover-validation.js << 'EOF'
const { MongoClient } = require('mongodb');

class FailoverValidator {
  constructor(originalEndpoint, failoverEndpoint, credentials) {
    this.originalClient = new MongoClient(`mongodb://${credentials.username}:${credentials.password}@${originalEndpoint}:27017/testDB?ssl=true&retryWrites=false`);
    this.failoverClient = new MongoClient(`mongodb://${credentials.username}:${credentials.password}@${failoverEndpoint}:27017/testDB?ssl=true&retryWrites=false`);
  }

  async validateFailover() {
    console.log('=== VALIDA√á√ÉO P√ìS-FAILOVER ===');
    
    try {
      // Conectar ao cluster de failover
      await this.failoverClient.connect();
      console.log('‚úÖ Conex√£o com cluster de failover estabelecida');
      
      // Executar testes de valida√ß√£o
      const results = {
        connectivity: await this.testConnectivity(),
        dataIntegrity: await this.testDataIntegrity(),
        performance: await this.testPerformance(),
        functionality: await this.testFunctionality()
      };
      
      // Gerar relat√≥rio
      this.generateReport(results);
      
      return results;
      
    } catch (error) {
      console.error('‚ùå Erro na valida√ß√£o:', error);
      throw error;
    } finally {
      await this.failoverClient.close();
    }
  }

  async testConnectivity() {
    console.log('\n1. Testando conectividade...');
    
    try {
      await this.failoverClient.db('admin').command({ ping: 1 });
      console.log('   ‚úÖ Ping bem-sucedido');
      
      const serverStatus = await this.failoverClient.db('admin').command({ serverStatus: 1 });
      console.log(`   ‚úÖ Vers√£o do servidor: ${serverStatus.version}`);
      
      return { status: 'PASS', details: 'Conectividade OK' };
    } catch (error) {
      console.log('   ‚ùå Falha na conectividade:', error.message);
      return { status: 'FAIL', details: error.message };
    }
  }

  async testDataIntegrity() {
    console.log('\n2. Testando integridade dos dados...');
    
    try {
      const db = this.failoverClient.db('performanceDB');
      
      // Contar documentos em collections principais
      const collections = ['products', 'orders'];
      const counts = {};
      
      for (const collName of collections) {
        try {
          const count = await db.collection(collName).countDocuments();
          counts[collName] = count;
          console.log(`   ‚úÖ ${collName}: ${count} documentos`);
        } catch (error) {
          console.log(`   ‚ö†Ô∏è  ${collName}: Erro ao contar - ${error.message}`);
          counts[collName] = -1;
        }
      }
      
      // Verificar √≠ndices
      for (const collName of collections) {
        try {
          const indexes = await db.collection(collName).indexes();
          console.log(`   ‚úÖ ${collName}: ${indexes.length} √≠ndices`);
        } catch (error) {
          console.log(`   ‚ö†Ô∏è  ${collName}: Erro ao verificar √≠ndices - ${error.message}`);
        }
      }
      
      return { status: 'PASS', details: counts };
    } catch (error) {
      console.log('   ‚ùå Falha na verifica√ß√£o de integridade:', error.message);
      return { status: 'FAIL', details: error.message };
    }
  }

  async testPerformance() {
    console.log('\n3. Testando performance...');
    
    try {
      const db = this.failoverClient.db('performanceDB');
      const collection = db.collection('products');
      
      // Teste de leitura
      const readStart = Date.now();
      await collection.findOne({});
      const readTime = Date.now() - readStart;
      
      // Teste de escrita
      const writeStart = Date.now();
      await collection.insertOne({
        _id: `failover-test-${Date.now()}`,
        timestamp: new Date(),
        test: 'failover-validation'
      });
      const writeTime = Date.now() - writeStart;
      
      // Teste de query complexa
      const queryStart = Date.now();
      await collection.find({ category: 'electronics' }).limit(10).toArray();
      const queryTime = Date.now() - queryStart;
      
      console.log(`   ‚úÖ Leitura: ${readTime}ms`);
      console.log(`   ‚úÖ Escrita: ${writeTime}ms`);
      console.log(`   ‚úÖ Query: ${queryTime}ms`);
      
      return {
        status: 'PASS',
        details: {
          readLatency: readTime,
          writeLatency: writeTime,
          queryLatency: queryTime
        }
      };
    } catch (error) {
      console.log('   ‚ùå Falha no teste de performance:', error.message);
      return { status: 'FAIL', details: error.message };
    }
  }

  async testFunctionality() {
    console.log('\n4. Testando funcionalidades...');
    
    try {
      const db = this.failoverClient.db('performanceDB');
      const testCollection = db.collection('failover_test');
      
      // CRUD operations
      const testDoc = {
        _id: `test-${Date.now()}`,
        timestamp: new Date(),
        data: 'failover-test'
      };
      
      // Create
      await testCollection.insertOne(testDoc);
      console.log('   ‚úÖ INSERT funcionando');
      
      // Read
      const found = await testCollection.findOne({ _id: testDoc._id });
      if (found) {
        console.log('   ‚úÖ FIND funcionando');
      }
      
      // Update
      await testCollection.updateOne(
        { _id: testDoc._id },
        { $set: { updated: new Date() } }
      );
      console.log('   ‚úÖ UPDATE funcionando');
      
      // Delete
      await testCollection.deleteOne({ _id: testDoc._id });
      console.log('   ‚úÖ DELETE funcionando');
      
      // Aggregation
      const aggResult = await testCollection.aggregate([
        { $match: {} },
        { $count: 'total' }
      ]).toArray();
      console.log('   ‚úÖ AGGREGATION funcionando');
      
      return { status: 'PASS', details: 'Todas as opera√ß√µes CRUD funcionando' };
    } catch (error) {
      console.log('   ‚ùå Falha no teste de funcionalidade:', error.message);
      return { status: 'FAIL', details: error.message };
    }
  }

  generateReport(results) {
    console.log('\n=== RELAT√ìRIO DE VALIDA√á√ÉO ===');
    
    const allPassed = Object.values(results).every(r => r.status === 'PASS');
    
    console.log(`Status Geral: ${allPassed ? '‚úÖ APROVADO' : '‚ùå REPROVADO'}`);
    console.log('\nDetalhes por Teste:');
    
    Object.entries(results).forEach(([test, result]) => {
      const status = result.status === 'PASS' ? '‚úÖ' : '‚ùå';
      console.log(`${status} ${test.toUpperCase()}: ${result.status}`);
    });
    
    if (results.performance.status === 'PASS') {
      console.log('\nM√©tricas de Performance:');
      console.log(`- Lat√™ncia de Leitura: ${results.performance.details.readLatency}ms`);
      console.log(`- Lat√™ncia de Escrita: ${results.performance.details.writeLatency}ms`);
      console.log(`- Lat√™ncia de Query: ${results.performance.details.queryLatency}ms`);
    }
    
    console.log('\n=== FIM DO RELAT√ìRIO ===');
  }
}

// CLI interface
async function main() {
  const originalEndpoint = process.argv[2];
  const failoverEndpoint = process.argv[3];
  
  if (!originalEndpoint || !failoverEndpoint) {
    console.log('Uso: node post-failover-validation.js <original-endpoint> <failover-endpoint>');
    process.exit(1);
  }
  
  const credentials = {
    username: process.env.DB_USERNAME,
    password: process.env.DB_PASSWORD
  };
  
  const validator = new FailoverValidator(originalEndpoint, failoverEndpoint, credentials);
  
  try {
    await validator.validateFailover();
    console.log('\n‚úÖ Valida√ß√£o conclu√≠da com sucesso!');
  } catch (error) {
    console.error('\n‚ùå Valida√ß√£o falhou:', error.message);
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = FailoverValidator;
EOF
```

---

## üí∞ Parte 5: An√°lise de Custos Cross-Region

### Passo 1: Calculadora de Custos

```bash
# Criar an√°lise de custos
cat > architectures/cost-optimization.md << 'EOF'
# An√°lise de Custos - Estrat√©gias Cross-Region

## Custos por Estrat√©gia

### 1. Snapshot Cross-Region (Disaster Recovery)

#### Componentes de Custo
- **Snapshots Storage:** $0.095/GB/m√™s (regi√£o secund√°ria)
- **Data Transfer:** $0.02/GB (cross-region)
- **Compute (standby):** $0 (apenas quando ativado)

#### Exemplo Mensal (Cluster 100GB)
```
Snapshot Storage: 100GB √ó $0.095 = $9.50/m√™s
Daily Transfer: 5GB √ó 30 dias √ó $0.02 = $3.00/m√™s
Total: ~$12.50/m√™s
```

### 2. Dual-Write (Active-Active)

#### Componentes de Custo
- **Primary Cluster:** db.t3.medium √ó 3 = ~$150/m√™s
- **Secondary Cluster:** db.t3.medium √ó 3 = ~$150/m√™s
- **Data Transfer:** ~$20/m√™s
- **Application Compute:** ~$50/m√™s

#### Total Mensal
```
Total: ~$370/m√™s (3x mais caro que single region)
```

### 3. Change Data Capture (CDC)

#### Componentes de Custo
- **Primary Cluster:** ~$150/m√™s
- **Secondary Cluster:** ~$150/m√™s
- **Lambda Executions:** ~$10/m√™s
- **Kinesis/DynamoDB:** ~$30/m√™s
- **Data Transfer:** ~$15/m√™s

#### Total Mensal
```
Total: ~$355/m√™s
```

## Otimiza√ß√µes de Custo

### Para Snapshot Strategy
1. **Snapshot Frequency:** Reduzir para 2x/dia
2. **Retention:** Manter apenas 7 dias
3. **Compression:** Usar compress√£o nos snapshots
4. **Lifecycle:** Mover para Glacier ap√≥s 30 dias

### Para Active-Active
1. **Instance Sizing:** Usar inst√¢ncias menores na regi√£o secund√°ria
2. **Read Replicas:** Reduzir n√∫mero de replicas
3. **Reserved Instances:** Usar RIs para economia de 30-60%

### Para CDC
1. **Batch Processing:** Agrupar mudan√ßas para reduzir execu√ß√µes Lambda
2. **Filtering:** Sincronizar apenas dados cr√≠ticos
3. **Compression:** Comprimir dados em tr√¢nsito

## ROI Analysis

### Custo de Downtime
- **E-commerce:** $5,000-50,000/hora
- **SaaS:** $1,000-10,000/hora
- **Enterprise:** $10,000-100,000/hora

### Break-even Analysis
```
Se downtime custa $10,000/hora:
- 1 hora de downtime evitada = ROI de 2-3 anos
- 4 horas de downtime evitadas = ROI de 6-12 meses
```

## Recomenda√ß√µes por Cen√°rio

### Startup/SMB
- **Estrat√©gia:** Snapshot Cross-Region
- **Custo:** ~$15/m√™s
- **RTO/RPO:** 2-4h / 1-24h

### Enterprise
- **Estrat√©gia:** CDC ou Active-Active
- **Custo:** ~$350/m√™s
- **RTO/RPO:** <5min / <1min

### Critical Systems
- **Estrat√©gia:** Active-Active + Monitoring
- **Custo:** ~$500/m√™s
- **RTO/RPO:** <1min / <30s
EOF
```

---

## ‚úÖ Checklist de Conclus√£o

Execute o script de valida√ß√£o:

```bash
# Executa o grade para avaliar atividades
./grade_exercicio4.sh
```

### Itens Verificados:

- ‚úÖ Limita√ß√µes do DocumentDB documentadas
- ‚úÖ Infraestrutura cross-region configurada
- ‚úÖ Automa√ß√£o de backup cross-region implementada
- ‚úÖ Estrat√©gia de sincroniza√ß√£o customizada desenvolvida
- ‚úÖ Plano de failover regional criado e testado
- ‚úÖ An√°lise de custos realizada

---

## üßπ Limpeza

```bash
# Deletar recursos na regi√£o secund√°ria
aws docdb delete-db-instance --region $SECONDARY_REGION --db-instance-identifier $FAILOVER_CLUSTER_ID-1 --skip-final-snapshot
aws docdb delete-db-cluster --region $SECONDARY_REGION --db-cluster-identifier $FAILOVER_CLUSTER_ID --skip-final-snapshot

# Deletar snapshots cross-region
aws docdb describe-db-cluster-snapshots --region $SECONDARY_REGION --query "DBClusterSnapshots[?contains(DBClusterSnapshotIdentifier, '$CLUSTER_ID-cross-region')].DBClusterSnapshotIdentifier" --output text | xargs -I {} aws docdb delete-db-cluster-snapshot --region $SECONDARY_REGION --db-cluster-snapshot-identifier {}

# Deletar infraestrutura secund√°ria
aws docdb delete-db-subnet-group --region $SECONDARY_REGION --db-subnet-group-name $ID-docdb-subnet-group-secondary
aws ec2 delete-subnet --region $SECONDARY_REGION --subnet-id $(aws ec2 describe-subnets --region $SECONDARY_REGION --filters "Name=vpc-id,Values=$SECONDARY_VPC_ID" --query 'Subnets[0].SubnetId' --output text)
aws ec2 delete-subnet --region $SECONDARY_REGION --subnet-id $(aws ec2 describe-subnets --region $SECONDARY_REGION --filters "Name=vpc-id,Values=$SECONDARY_VPC_ID" --query 'Subnets[1].SubnetId' --output text)
aws ec2 delete-vpc --region $SECONDARY_REGION --vpc-id $SECONDARY_VPC_ID

# Deletar fun√ß√£o Lambda
aws lambda delete-function --region $PRIMARY_REGION --function-name $ID-CrossRegionBackup

# Deletar regra EventBridge
aws events remove-targets --region $PRIMARY_REGION --rule $ID-cross-region-backup --ids 1
aws events delete-rule --region $PRIMARY_REGION --name $ID-cross-region-backup
```

---

## üìä Resumo das Estrat√©gias Implementadas

### Estrat√©gias Cross-Region Desenvolvidas:

1. **Snapshot Cross-Region:**
   - RPO: 1-24 horas
   - RTO: 2-4 horas
   - Custo: Baixo (~$15/m√™s)
   - Complexidade: M√©dia

2. **Sincroniza√ß√£o Customizada (CDC):**
   - RPO: Minutos
   - RTO: Minutos
   - Custo: Alto (~$350/m√™s)
   - Complexidade: Alta

3. **Failover Regional Automatizado:**
   - Detec√ß√£o autom√°tica de falhas
   - Restaura√ß√£o automatizada
   - Valida√ß√£o p√≥s-failover
   - Notifica√ß√µes integradas

### Limita√ß√µes Identificadas:

- ‚ùå Sem replica√ß√£o cross-region nativa
- ‚ùå Sem failover autom√°tico entre regi√µes
- ‚ùå Depend√™ncia de solu√ß√µes customizadas
- ‚ö†Ô∏è Custos elevados para alta disponibilidade

### Alternativas Recomendadas:

- **Para DR:** Snapshots cross-region automatizados
- **Para HA:** Arquitetura dual-write com conflict resolution
- **Para Performance:** CDN + edge caching
- **Para Compliance:** Backup multi-regi√£o com reten√ß√£o longa

---

[‚¨ÖÔ∏è Exerc√≠cio 3](../exercicio3-export-s3/README.md) | [üè† M√≥dulo 5 Home](../README.md)